{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from catalyst import dl\n",
    "from catalyst.dl import utils\n",
    "from catalyst.callbacks.scheduler import SchedulerCallback\n",
    "import catalyst.metrics.functional as CF\n",
    "from catalyst.contrib.callbacks.neptune_logger import NeptuneLogger\n",
    "from catalyst.contrib.nn.schedulers.onecycle import OneCycleLRWithWarmup\n",
    "from catalyst.callbacks.metric import LoaderMetricCallback\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "from catalyst.utils import set_global_seed\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_global_seed(12345786)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_map = {\n",
    "    'BI-Monthly': 24/12,\n",
    "    'Monthly': 12/12,\n",
    "    'Quatrly': 4/12,\n",
    "    'Half Yearly': 2/12\n",
    "}\n",
    "\n",
    "def bureausplit_dates(x):\n",
    "    if x == '':\n",
    "        return None\n",
    "    if isinstance(x, float):\n",
    "        return np.nan\n",
    "    dates = x.split(',')\n",
    "    if len(dates) == 1:\n",
    "        return None\n",
    "    else:\n",
    "        d = dates[-2]\n",
    "        return f\"{d[:4]}-{d[4:6]}-{d[6:8]}\"        \n",
    "    \n",
    "\n",
    "def correct_bureau_data(df):\n",
    "    df[\"DISBURSED-DT2\"] = pd.to_datetime(df[\"REPORTED DATE - HIST\"].apply(bureausplit_dates), errors='ignore')\n",
    "    return df\n",
    "\n",
    "def get_min_date(x):\n",
    "    loc = x[\"max_diff_loc\"] + 1\n",
    "    dates = str(x[\"REPORTED DATE - HIST\"])\n",
    "    if dates == 'nan':\n",
    "        return \"1970-01-01\"\n",
    "    else:\n",
    "        dates = dates.split(\",\")[:-1][::-1]\n",
    "        if len(dates) - 1 < loc:\n",
    "            return dates[-1]\n",
    "        min_date = dates[loc]\n",
    "        return f\"{min_date[:4]}-{min_date[4:6]}-{min_date[6:8]}\"\n",
    "\n",
    "\n",
    "def process_bal(x):\n",
    "    if x == 'nan':\n",
    "        return [0]\n",
    "    xl = x.split(\",\")[:-1][::-1]\n",
    "    prev_val = 0\n",
    "    vals = []\n",
    "    for val in xl:\n",
    "        if val == '':\n",
    "            val = prev_val\n",
    "            vals.append(val)\n",
    "        else:\n",
    "            val = int(val)\n",
    "            vals.append(val)\n",
    "            prev_val = val\n",
    "    return np.array(vals)\n",
    "\n",
    "\n",
    "def get_loan_frequency(bur):\n",
    "    mapping = {\n",
    "        \"Monthly\": 12/12,\n",
    "        \"Quarterly\": 3/12,\n",
    "        \"Weekly\": 12*4.33/12,\n",
    "        \"BiWeekly\": 2.16,\n",
    "        \"Other\": -1,\n",
    "        \"BiMonthly\": 2/12,\n",
    "        \"Annually\": 1/12,\n",
    "        \"Semi annually\": 6/12,\n",
    "        \"Single Payment Loan (bullet\": 0\n",
    "    }\n",
    "    bur[\"frequency\"] = bur[\"INSTALLMENT-AMT\"].str.split(\"/\").str.get(1).map(mapping)\n",
    "    bur[\"installment\"] = bur[\"INSTALLMENT-AMT\"].str.split(\"/\").str.get(0).str.replace(\",\", \"\").astype(float)\n",
    "    bur[\"installment_pm\"] = bur[\"installment\"] * bur[\"frequency\"]\n",
    "\n",
    "    bur[\"interest\"] = bur[\"TENURE\"] * bur[\"installment\"] * bur[\"frequency\"]/ bur[\"DISBURSED-AMT/HIGH CREDIT\"] \n",
    "    return bur\n",
    "\n",
    "\n",
    "def read_files(data_path):\n",
    "    train = pd.read_csv(Path(DATA_FOLDER) / Files.train_csv, thousands=\",\")\n",
    "    test = pd.read_csv(Path(DATA_FOLDER) / Files.test_csv, thousands=\",\")\n",
    "    \n",
    "    train[\"DisbursalDate\"] = pd.to_datetime(train[\"DisbursalDate\"], errors='coerce')\n",
    "    test[\"DisbursalDate\"] = pd.to_datetime(test[\"DisbursalDate\"], errors='coerce')\n",
    "\n",
    "    train[\"MaturityDAte\"] = pd.to_datetime(train[\"MaturityDAte\"], errors='coerce')\n",
    "    test[\"MaturityDAte\"] = pd.to_datetime(test[\"MaturityDAte\"], errors='coerce')\n",
    "\n",
    "    train[\"AuthDate\"] = pd.to_datetime(train[\"AuthDate\"], errors='coerce')\n",
    "    test[\"AuthDate\"] = pd.to_datetime(test[\"AuthDate\"], errors='coerce')\n",
    "\n",
    "    train[\"Frequency\"] = train[\"Frequency\"].map(freq_map)\n",
    "    test[\"Frequency\"] = test[\"Frequency\"].map(freq_map)\n",
    "\n",
    "    train[\"days_to_enddate\"] = (pd.to_datetime(\"2020-12-01\") - train[\"DisbursalDate\"]).dt.days\n",
    "    test[\"days_to_enddate\"] = (pd.to_datetime(\"2020-12-01\") - test[\"DisbursalDate\"]).dt.days\n",
    "\n",
    "    train[\"days_from_start\"] =  (train[\"DisbursalDate\"] - pd.to_datetime(\"2010-12-01\")).dt.days\n",
    "    test[\"days_from_start\"] = (test[\"DisbursalDate\"] - pd.to_datetime(\"2010-12-01\")).dt.days\n",
    "\n",
    "    train_bureau = pd.read_csv(Path(DATA_FOLDER) / Files.train_bureau_csv, thousands=\",\")\n",
    "    test_bureau = pd.read_csv(Path(DATA_FOLDER) / Files.test_bureau_csv, thousands=\",\")\n",
    "    \n",
    "    data = pd.concat([train, test])\n",
    "    \n",
    "    bureau = pd.concat([train_bureau, test_bureau]).reset_index(drop=True)\n",
    "    bureau[\"DISBURSED-DT\"] = pd.to_datetime(bureau[\"DISBURSED-DT\"])\n",
    "    # bureau = bureau.drop_duplicates(subset=[\"ID\", \"DISBURSED-DT\"])\n",
    "    bureau = bureau.sort_values(by=[\"ID\", \"DISBURSED-DT\"])\n",
    "    bureau = pd.merge(bureau, data[[\"ID\", \"DisbursalDate\", \"Tenure\", \"DisbursalAmount\", \"AssetCost\", \"EMI\", \"MonthlyIncome\", \"MaturityDAte\", \"ZiPCODE\", \"Area\", \"State\"]], on=\"ID\", how=\"left\")\n",
    "    bureau = correct_bureau_data(bureau)\n",
    "    bureau.loc[bureau[\"DISBURSED-DT\"].isnull(), \"DISBURSED-DT\"] = pd.to_datetime(bureau.loc[bureau[\"DISBURSED-DT\"].isnull(), \"DISBURSED-DT2\"])\n",
    "    bureau[\"post_days\"] = (bureau[\"DISBURSED-DT\"] - bureau[\"DisbursalDate\"]).dt.days\n",
    "    bureau[\"close_days\"] = (pd.to_datetime(bureau[\"CLOSE-DT\"], errors='coerce') - pd.to_datetime(bureau[\"DisbursalDate\"])).dt.days\n",
    "    bureau[\"days_to_enddate\"] = (pd.to_datetime(\"2020-12-01\") - bureau[\"DISBURSED-DT\"]).dt.days\n",
    "    bureau[\"post_end_days_diff\"] = bureau[\"days_to_enddate\"] - bureau[\"post_days\"]\n",
    "    bureau[\"post_days_diff\"] = bureau.groupby(\"ID\")[\"post_days\"].transform(lambda x: x.duplicated(keep=False))\n",
    "    bureau[\"cur_bal_int\"] = bureau[\"CUR BAL - HIST\"].astype(str).apply(process_bal)  # str.split(',').apply(lambda x: [int(val) if val != '' else np.nan for w, val in enumerate(x)])\n",
    "    bureau[\"max_diff\"] = bureau[\"cur_bal_int\"].apply(lambda x:  np.diff(x).max() if len(x) > 1 else 0)\n",
    "    bureau[\"max_diff_loc\"] = bureau[\"cur_bal_int\"].apply(lambda x:  np.argmax(np.diff(x)) if len(x) > 1 else 0)\n",
    "    bureau[\"max_diff_date\"] = pd.to_datetime(bureau[[\"REPORTED DATE - HIST\", \"max_diff_loc\"]].apply(get_min_date, axis=1), errors=\"coerce\")\n",
    "    bureau[\"days_to_max_diff\"] = (bureau[\"max_diff_date\"] - bureau[\"DisbursalDate\"]).dt.days\n",
    "    bureau = get_loan_frequency(bureau)\n",
    "\n",
    "    train[\"ACCT-TYPE\"] = train.ID.map(bureau.loc[(bureau[\"SELF-INDICATOR\"] == True) & (bureau[\"post_days\"] >= 0)].groupby(\"ID\")[\"ACCT-TYPE\"].first())\n",
    "    test[\"ACCT-TYPE\"] = test.ID.map(bureau.loc[(bureau[\"SELF-INDICATOR\"] == True) & (bureau[\"post_days\"] >= 0)].groupby(\"ID\")[\"ACCT-TYPE\"].first())\n",
    "\n",
    "    train[\"ownership\"] = train.ID.map(bureau.loc[(bureau[\"SELF-INDICATOR\"] == True) & (bureau[\"post_days\"] >= 0)].groupby(\"ID\")[\"OWNERSHIP-IND\"].first())\n",
    "    test[\"ownership\"] = test.ID.map(bureau.loc[(bureau[\"SELF-INDICATOR\"] == True) & (bureau[\"post_days\"] >= 0)].groupby(\"ID\")[\"OWNERSHIP-IND\"].first())\n",
    "\n",
    "    train[\"status\"] = train.ID.map(bureau.loc[(bureau[\"SELF-INDICATOR\"] == True) & (bureau[\"post_days\"] >= 0)].groupby(\"ID\")[\"ACCOUNT-STATUS\"].first())\n",
    "    test[\"status\"] = test.ID.map(bureau.loc[(bureau[\"SELF-INDICATOR\"] == True) & (bureau[\"post_days\"] >= 0)].groupby(\"ID\")[\"ACCOUNT-STATUS\"].first())\n",
    "    \n",
    "    train[\"disbursed_bur\"] = train.ID.map(bureau.loc[(bureau[\"SELF-INDICATOR\"] == True) & (bureau[\"post_days\"] >= 0)].groupby(\"ID\")[\"DISBURSED-AMT/HIGH CREDIT\"].first())\n",
    "    test[\"disbursed_bur\"] = test.ID.map(bureau.loc[(bureau[\"SELF-INDICATOR\"] == True) & (bureau[\"post_days\"] >= 0)].groupby(\"ID\")[\"DISBURSED-AMT/HIGH CREDIT\"].first())\n",
    "\n",
    "    train[\"CLOSE-DT\"] = pd.to_datetime(train.ID.map(bureau.loc[(bureau[\"SELF-INDICATOR\"] == True) & (bureau[\"post_days\"] >= 0)].groupby(\"ID\")[\"CLOSE-DT\"].first()), errors='coerce')\n",
    "    test[\"CLOSE-DT\"] = pd.to_datetime(test.ID.map(bureau.loc[(bureau[\"SELF-INDICATOR\"] == True) & (bureau[\"post_days\"] >= 0)].groupby(\"ID\")[\"CLOSE-DT\"].first()), errors='coerce')\n",
    "\n",
    "    train[\"DATE-REPORTED\"] = pd.to_datetime(train.ID.map(bureau.loc[(bureau[\"SELF-INDICATOR\"] == True) & (bureau[\"post_days\"] >= 0)].groupby(\"ID\")[\"DATE-REPORTED\"].first()), errors='coerce')\n",
    "    test[\"DATE-REPORTED\"] = pd.to_datetime(test.ID.map(bureau.loc[(bureau[\"SELF-INDICATOR\"] == True) & (bureau[\"post_days\"] >= 0)].groupby(\"ID\")[\"DATE-REPORTED\"].first()), errors='coerce')\n",
    "\n",
    "    train[\"DISBURSED-DT\"] = pd.to_datetime(train.ID.map(bureau.loc[(bureau[\"SELF-INDICATOR\"] == True) & (bureau[\"post_days\"] >= 0)].groupby(\"ID\")[\"DISBURSED-DT\"].first()), errors='coerce')\n",
    "    test[\"DISBURSED-DT\"] = pd.to_datetime(test.ID.map(bureau.loc[(bureau[\"SELF-INDICATOR\"] == True) & (bureau[\"post_days\"] >= 0)].groupby(\"ID\")[\"DISBURSED-DT\"].first()), errors='coerce')\n",
    "\n",
    "    train[\"max_diff\"] = train.ID.map(bureau.loc[(bureau[\"SELF-INDICATOR\"] == True) & (bureau[\"post_days\"] >= 0)].groupby(\"ID\")[\"max_diff\"].first())\n",
    "    test[\"max_diff\"] = test.ID.map(bureau.loc[(bureau[\"SELF-INDICATOR\"] == True) & (bureau[\"post_days\"] >= 0)].groupby(\"ID\")[\"max_diff\"].first())\n",
    "\n",
    "    train[\"days_to_max_diff\"] = train.ID.map(bureau.loc[(bureau[\"SELF-INDICATOR\"] == True) & (bureau[\"post_days\"] >= 0)].groupby(\"ID\")[\"days_to_max_diff\"].first())\n",
    "    test[\"days_to_max_diff\"] = test.ID.map(bureau.loc[(bureau[\"SELF-INDICATOR\"] == True) & (bureau[\"post_days\"] >= 0)].groupby(\"ID\")[\"days_to_max_diff\"].first())\n",
    "\n",
    "    return train, test, bureau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"data\"\n",
    "\n",
    "\n",
    "class Files:\n",
    "    \"\"\"Easy retrieval of file paths.\"\"\"\n",
    "    train = \"Train/train_Data.xlsx\"\n",
    "    test = \"test_Data.xlsx\"\n",
    "    data_dict = \"Train/data_dict.xlsx\"\n",
    "    train_csv = \"train_Data.csv\"\n",
    "    test_csv = \"test_Data.csv\"\n",
    "    train_bureau = \"Train/train_bureau.xlsx\"\n",
    "    test_bureau = \"test_bureau.xlsx\"\n",
    "    train_bureau_csv = \"train_bureau.csv\"\n",
    "    test_bureau_csv = \"test_bureau.csv\"\n",
    "\n",
    "\n",
    "TargetMap = {\n",
    "    \"No Top-up Service\": 6,\n",
    "    \" > 48 Months\": 5,\n",
    "    \"36-48 Months\": 4,\n",
    "    \"30-36 Months\": 3,\n",
    "    \"24-30 Months\": 2,\n",
    "    \"18-24 Months\": 1,\n",
    "    \"12-18 Months\": 0\n",
    "}\n",
    "\n",
    "TargetRevMap = {v: k for k, v in TargetMap.items()}\n",
    "\n",
    "#train, test, bureau = read_files(\"data\")\n",
    "# train.to_parquet(\"data/train_v1.pq\",)\n",
    "# test.to_parquet(\"data/test_v1.pq\",)\n",
    "# bureau.to_parquet(\"data/bureau_v1.pq\",)\n",
    "\n",
    "train = pd.read_parquet(\"data/train_v1.pq\")\n",
    "test = pd.read_parquet(\"data/test_v1.pq\")\n",
    "bureau = pd.read_parquet(\"data/bureau_v1.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def more_cat_features(df):\n",
    "    df[\"close_date_diff\"] = (df[\"CLOSE-DT\"] - df[\"MaturityDAte\"]).dt.days\n",
    "    df[\"close_disburse_diff\"] = (df[\"CLOSE-DT\"] - df[\"DisbursalDate\"]).dt.days\n",
    "    df[\"reported_maturity_diff\"] = (df[\"DATE-REPORTED\"] - df[\"MaturityDAte\"]).dt.days\n",
    "    return df\n",
    "\n",
    "train = more_cat_features(train)\n",
    "test = more_cat_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "def prepare_bureau_data(bur_df, max_len=15):\n",
    "    bur_df = bur_df.copy()\n",
    "    cat_cols = [\"ACCT-TYPE\", \"OWNERSHIP-IND\", \"CONTRIBUTOR-TYPE\", \"ACCOUNT-STATUS\"]\n",
    "    cont_cols = [\"DISBURSED-AMT/HIGH CREDIT\", \"post_days\", \"close_days\", \"TENURE\", \"CURRENT-BAL\"]\n",
    "    cat_maps = {col: {val: i+1 for i, val in enumerate(bur_df[col].unique())} for col in cat_cols}\n",
    "    cat_levels = {cat: len(cat_d) for cat, cat_d in cat_maps.items()}\n",
    "    \n",
    "    medians = bur_df[cont_cols].median()\n",
    "    bur_qnt = QuantileTransformer(output_distribution=\"normal\")\n",
    "    bur_qnt.fit(bur_df[cont_cols].fillna(medians))\n",
    "    for col in cat_cols:\n",
    "        bur_df[f\"{col}_enc\"] = bur_df[col].map(cat_maps[col])\n",
    "    \n",
    "    bur_cats = []\n",
    "    bur_conts = []\n",
    "    bur_df = bur_df.loc[bur_df.post_days > -732]\n",
    "    bur_ids = set(bur_df.ID.unique())\n",
    "    for bid in tqdm(range(bur_df.ID.max())):\n",
    "        if (bid in bur_ids):\n",
    "            sub_df = bur_df.loc[bur_df.ID == bid]\n",
    "            cat_encs = []\n",
    "            for col in cat_cols:\n",
    "                enc = sub_df[f\"{col}_enc\"].values\n",
    "                cat_encs.append(enc)\n",
    "            cat_encs = pad_sequences(np.vstack(cat_encs), max_len, 'int64', 'post', 'pre', 0)\n",
    "            bur_cats.append(cat_encs)\n",
    "            conts = pad_sequences(bur_qnt.transform(sub_df[cont_cols].fillna(medians)).T, max_len, 'float32', 'post', 'pre', 0)/2.0\n",
    "            bur_conts.append(conts)\n",
    "        else:\n",
    "            cat_encs = np.zeros(shape=(len(cat_cols), max_len), dtype='int64')\n",
    "            bur_cats.append(cat_encs)\n",
    "            conts = np.zeros(shape=(len(cont_cols), max_len), dtype='float32')\n",
    "            bur_conts.append(conts)\n",
    "        \n",
    "    return np.moveaxis(np.moveaxis(np.dstack(bur_cats), 2, 0), 1, 2), np.moveaxis(np.moveaxis(np.dstack(bur_conts), 2, 0), 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "def prepare_data_tr_te(train, test):\n",
    "    cont_features = ['Tenure', 'AssetCost', 'AmountFinance', 'DisbursalAmount', 'EMI', 'MonthlyIncome', 'AGE', 'LTV',\n",
    "                            \"Frequency\", \"days_to_enddate\", 'disbursed_bur', 'days_from_start', 'close_date_diff', 'close_disburse_diff',\n",
    "                     'reported_maturity_diff']\n",
    "    cat_features = [\"InstlmentMode\", \"LoanStatus\", \"PaymentMode\", \"SEX\", \"State\", \"Area\", \"ManufacturerID\", \"BranchID\", \"SupplierID\"]\n",
    "    \n",
    "    data = pd.concat([train, test]).reset_index(drop=True)\n",
    "    medians = data[cont_features].median()\n",
    "    cont_enc = PowerTransformer()\n",
    "    data[cont_features] = cont_enc.fit_transform(data[cont_features].fillna(medians))/2.0\n",
    "\n",
    "    cat_maps = {col: {val: i+1 for i, val in enumerate(data[col].astype(str).unique())} for col in cat_features}\n",
    "    for col in cat_features:\n",
    "        data[col] = data[col].astype(str).map(cat_maps[col])\n",
    "    data = data.set_index(\"ID\")\n",
    "    data.to_parquet(\"data/data_prepared.pq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bureau_cats, bureau_conts = prepare_bureau_data(bureau, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"data/bureau_cats.npy\", bureau_cats)\n",
    "#np.save(\"data/bureau_conts.npy\", bureau_conts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare_data_tr_te(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordinal_encode(x):\n",
    "    if (x[\"Top-up Month\"] == 'No Top-up Service'):\n",
    "        return [0, 0, 0, 0, 0, 0]\n",
    "    elif x[\"Top-up Month\"] == '12-18 Months':\n",
    "        return [1, 0, 0, 0, 0, 0]\n",
    "    elif x[\"Top-up Month\"] == '18-24 Months':\n",
    "        return [1, 1, 0, 0, 0, 0]\n",
    "    elif x[\"Top-up Month\"] == '24-30 Months':\n",
    "        return [1, 1, 1, 0, 0, 0]\n",
    "    elif x[\"Top-up Month\"] == '30-36 Months':\n",
    "        return [1, 1, 1, 1, 0, 0]\n",
    "    elif x[\"Top-up Month\"] == '36-48 Months':\n",
    "        return [1, 1, 1, 1, 1, 0]\n",
    "    elif x[\"Top-up Month\"] == ' > 48 Months':\n",
    "        return [1, 1, 1, 1 ,1, 1]\n",
    "    else:\n",
    "        return [0, 0, 0, 0, 0, 0]\n",
    "\n",
    "class LoanData(Dataset):\n",
    "    def __init__(self, df, cont_features, cat_features, training=True, maxlen=20):\n",
    "        self.df = df\n",
    "        self.cont_features = cont_features\n",
    "        self.cat_features = cat_features\n",
    "        self.training = training\n",
    "        self.maxlen = maxlen\n",
    "        self.y = None\n",
    "        self.data = None\n",
    "        self.prepare_data()\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        self.ids = self.df.ID.values\n",
    "        self.data = pd.read_parquet(\"data/data_prepared.pq\")\n",
    "        if self.training:\n",
    "            self.y = np.vstack(self.df.apply(ordinal_encode, axis=1)).astype(int)\n",
    "        self.bur_cats = np.load(\"data/bureau_cats.npy\")\n",
    "        self.bur_conts = np.load(\"data/bureau_conts.npy\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ID = self.ids[idx]\n",
    "        bur_cat, bur_cont = self.bur_cats[ID], self.bur_conts[ID]\n",
    "        if self.training:\n",
    "            return {\n",
    "                \"x_cat\": {cat: self.data.loc[ID, cat] for i, cat in enumerate(self.cat_features)},\n",
    "                \"x_cont\": self.data.loc[ID, self.cont_features].astype(np.float32).values,\n",
    "                \"bur_cat\": bur_cat,\n",
    "                \"bur_cont\": bur_cont.astype(np.float32)\n",
    "            }, self.y[idx].astype(np.float32)\n",
    "        else:\n",
    "            return {\n",
    "                \"x_cat\": {cat: self.data.loc[ID, cat] for i, cat in enumerate(self.cat_features)},\n",
    "                \"x_cont\": self.data.loc[ID, self.cont_features].astype(np.float32).values,\n",
    "                    \"bur_cat\": bur_cat,\n",
    "                    \"bur_cont\": bur_cont.astype(np.float32)\n",
    "                   },\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer, OrdinalEncoder, PowerTransformer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cvlist = list(KFold(5, shuffle=True, random_state=123995786).split(train))\n",
    "cat_features = [\"InstlmentMode\", \"LoanStatus\", \"PaymentMode\", \"SEX\", \"State\", \"Area\", \"ManufacturerID\", \"BranchID\", \"SupplierID\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, vl = train.iloc[cvlist[0][0]], train.iloc[cvlist[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((102924, 40), (25731, 40))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.shape, vl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 15)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_features = ['Tenure', 'AssetCost', 'AmountFinance', 'DisbursalAmount', 'EMI', 'MonthlyIncome', 'AGE', 'LTV',\n",
    "                        \"Frequency\", \"days_to_enddate\", 'disbursed_bur', 'days_from_start', 'close_date_diff', 'close_disburse_diff',\n",
    "                 'reported_maturity_diff']\n",
    "cat_features = [\"InstlmentMode\", \"LoanStatus\", \"PaymentMode\", \"SEX\", \"State\", \"Area\", \"ManufacturerID\", \"BranchID\", \"SupplierID\"]\n",
    "\n",
    "len(cat_features), len(cont_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoanModel(nn.Module):\n",
    "    def __init__(self, hparams, device):\n",
    "        super(LoanModel, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        # [2, 2, 11, 3, 22, 93, 10, 10, 5]\n",
    "        embeds = [8, 8, 8, 8, 16, 32, 16, 128, 128]\n",
    "        embeds2 = [100, 16, 16, 16]\n",
    "        bur_cont_dim = 512\n",
    "        cont_dim = 1024\n",
    "        max_len = 15\n",
    "        gru_dim = 128\n",
    "        compress_dim=4096\n",
    "        bidirectional=True\n",
    "        self.install_embed = nn.Embedding(4, embeds[0])\n",
    "        self.status1_embed = nn.Embedding(4, embeds[1])\n",
    "        self.payment_embed = nn.Embedding(13, embeds[2])\n",
    "        self.sex_embed = nn.Embedding(4, embeds[3])\n",
    "        self.state_embed = nn.Embedding(24, embeds[4])\n",
    "        self.area_embed = nn.Embedding(95, embeds[5], max_norm=1)\n",
    "        self.manu_embed = nn.Embedding(12, embeds[6])\n",
    "        self.branch_embed = nn.Embedding(192, embeds[7])\n",
    "        self.supp_embed = nn.Embedding(4542, embeds[8])\n",
    "    \n",
    "        self.acct_embed = nn.Embedding(52, embeds2[0])\n",
    "        self.own_embed = nn.Embedding(6, embeds2[1])\n",
    "        self.con_embed = nn.Embedding(13, embeds2[2])\n",
    "        self.status2_embed = nn.Embedding(12, embeds2[3])\n",
    "\n",
    "\n",
    "        self.cont_fc = nn.Linear(hparams[\"num_cont_features\"], cont_dim)\n",
    "        self.bur_fc = nn.Linear(5, bur_cont_dim)\n",
    "        self.num_features = cont_dim + sum(embeds[:-2])\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.attn = nn.Linear(self.num_features, self.num_features)\n",
    "        self.compress = nn.Linear(max_len*(gru_dim * (1 + bidirectional)), compress_dim)\n",
    "        self.attn2 = nn.Linear(compress_dim, compress_dim)\n",
    "        self.fc1 = nn.Linear(self.num_features + compress_dim, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 2048)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.fc4 = nn.Linear(2048, 6)\n",
    "        # self.gru = nn.Tra\n",
    "        self.gru = nn.GRU(sum(embeds2) + bur_cont_dim, gru_dim, bidirectional=bidirectional, num_layers=2, dropout=0.0)\n",
    "        self.relu = nn.GELU()\n",
    "        self.drop = nn.Dropout(0.0)\n",
    "        self.bnorm1 = nn.BatchNorm1d(5)\n",
    "        self.bnorm2 = nn.BatchNorm1d(hparams[\"num_cont_features\"])\n",
    "        self.batch_norm = nn.BatchNorm1d(self.num_features + compress_dim)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(2048)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        cats = x[\"x_cat\"]\n",
    "        cat_layers = []\n",
    "        c1 = self.install_embed(cats[\"InstlmentMode\"])\n",
    "        c2 = self.status1_embed(cats[\"LoanStatus\"])\n",
    "        c3 = self.payment_embed(cats[\"PaymentMode\"])\n",
    "        c4 = self.sex_embed(cats[\"SEX\"])\n",
    "        c5 = self.state_embed(cats[\"State\"])\n",
    "        c6 = self.area_embed(cats[\"Area\"])\n",
    "        c7 = self.manu_embed(cats[\"ManufacturerID\"])\n",
    "        c8 = self.branch_embed(cats[\"BranchID\"])\n",
    "        c9 = self.supp_embed(cats[\"SupplierID\"])\n",
    "        c12 = self.acct_embed(x[\"bur_cat\"][:, :, 0])\n",
    "        c13 = self.own_embed(x[\"bur_cat\"][:, :, 1])\n",
    "        c14 = self.con_embed(x[\"bur_cat\"][:, :, 2])\n",
    "        c15 = self.status2_embed(x[\"bur_cat\"][:, :, 3])\n",
    "        c16 = self.bur_fc(self.bnorm1(x[\"bur_cont\"].permute(0, 2, 1).contiguous()).permute(0, 2, 1).contiguous())\n",
    "        bur_out = torch.cat((c12, c13, c14, c15, c16), -1)\n",
    "        b1, _ = self.gru(bur_out)\n",
    "        bs = b1.size()[0]\n",
    "        b1 = b1.view(bs, -1).contiguous()\n",
    "        b1 = self.compress(b1)\n",
    "        b1 = b1 + self.tanh(b1) * self.attn2(b1)\n",
    "        conts = x[\"x_cont\"]\n",
    "        conts = self.cont_fc(self.bnorm2(conts))\n",
    "        out = torch.cat((c1, c2, c3, c4, c5, c6, c7, conts), -1)\n",
    "        out1 = self.tanh(out)\n",
    "        out2 = self.attn(out)\n",
    "        out = out + out1 * out2\n",
    "        out = torch.cat((out, b1), -1)\n",
    "#        out = self.batch_norm(self.relu(out))\n",
    "        out = self.relu(out)\n",
    "        out = self.relu(self.fc1(out))\n",
    "        out = self.drop(self.relu(self.fc2(out)))\n",
    "        #out = self.drop(self.relu(self.fc3(out)))\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, loader, cuda=True):\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    images = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            x, y = batch\n",
    "            if cuda:\n",
    "                outputs = nn.Sigmoid()(model(x))\n",
    "                preds.append(outputs.cpu().numpy())\n",
    "\n",
    "            else:\n",
    "                outputs = nn.Sigmoid()(model.cpu()(x))\n",
    "                preds.append(outputs.numpy())\n",
    "        preds = np.concatenate(preds)\n",
    "    return preds \n",
    "\n",
    "\n",
    "def train_folds(train, test, cvlist, hparams, device, logger, cont_features, cat_features):\n",
    "    val_preds = []\n",
    "    test_preds = []\n",
    "    for tr_idx, vl_idx in cvlist:\n",
    "        tr, vl = train.iloc[tr_idx], train.iloc[vl_idx]\n",
    "        tr_ds = LoanData(tr, cont_features, cat_features, training=True)\n",
    "        tr_dl = DataLoader(tr_ds, shuffle=True, drop_last=False, batch_size=hparams[\"batch_size\"], num_workers=8)\n",
    "\n",
    "        vl_ds = LoanData(vl, cont_features, cat_features, training=True)\n",
    "        vl_dl = DataLoader(vl_ds, shuffle=False, drop_last=False, batch_size=hparams[\"batch_size\"], num_workers=8)\n",
    "\n",
    "        te_ds = LoanData(test, cont_features, cat_features, training=False)\n",
    "        te_dl = DataLoader(te_ds, shuffle=False, drop_last=False, batch_size=hparams[\"batch_size\"], num_workers=8)\n",
    "\n",
    "        hparams[\"num_cont_features\"] = len(cont_features)    \n",
    "        # hparams[\"cat_features\"] = [(cat, len(cat_cats[i])+1, 8) for i, cat in enumerate(cat_features)]\n",
    "\n",
    "        model = LoanModel(hparams, device)\n",
    "        if hparams.get(\"optimizer\", \"adam\") == \"adam\":\n",
    "            optimizer = torch.optim.Adam(\n",
    "                model.parameters(), lr=hparams.get(\"lr\", 1e-3), weight_decay=hparams.get(\"wd\", 0), amsgrad=True,\n",
    "            )\n",
    "        else:\n",
    "            optimizer = torch.optim.SGD(\n",
    "                model.parameters(),\n",
    "                lr=hparams.get(\"lr\", 1e-2),\n",
    "                weight_decay=hparams.get(\"wd\", 0),\n",
    "                momentum=0.9,\n",
    "                nesterov=True,\n",
    "            )\n",
    "        if hparams.get(\"scheduler\", \"reducelrplateau\") == \"reducelrplateau\":\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True, factor=0.5, min_lr=1e-6)\n",
    "        if hparams.get(\"scheduler\", \"reducelrplateau\") == \"onecycle\":\n",
    "            total_steps = hparams.get(\"num_epochs\") * int(np.ceil(len(tr) / hparams.get(\"batch_size\")))\n",
    "            max_lr = hparams.get(\"lr\", 1e-3)\n",
    "            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, total_steps)\n",
    "        if hparams.get(\"scheduler\", \"reducelrplateau\") == \"steplr\":\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 3, 0.1)\n",
    "\n",
    "        logdir = Path(\"test\") / f\"fold_0\"\n",
    "        logdir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(1.0), reduction='mean')\n",
    "\n",
    "        runner = dl.SupervisedRunner(device=device)\n",
    "        runner.train(\n",
    "            loaders={\"train\": tr_dl, \"valid\": vl_dl},\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            num_epochs=hparams.get(\"num_epochs\", 10),\n",
    "            logdir=logdir,\n",
    "            verbose=True,\n",
    "            callbacks=[logger, SchedulerCallback(mode=\"epoch\"), LoaderMetricCallback(\"f1\", ordinal_f1_score)],\n",
    "            load_best_on_end=True,\n",
    "            main_metric='f1',\n",
    "            minimize_metric=True\n",
    "        )\n",
    "        vpreds = predict(model, vl_dl, False)\n",
    "        tpreds = predict(model, te_dl, False)\n",
    "        val_preds.append(vpreds)\n",
    "        test_preds.append(tpreds)\n",
    "    return model, val_preds, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10000\n",
    "hparams = {\n",
    "    \"scheduler\": \"reducelrplateau\",\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"num_epochs\": 20,\n",
    "    \"lr\": 5e-4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.metrics import fbeta_score\n",
    "def ordinal_f1_score(\n",
    "    outputs: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    eps: float = 1e-7,\n",
    "    argmax_dim: int = -1,\n",
    "    num_classes = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fbeta_score with beta=1.\n",
    "\n",
    "    Args:\n",
    "        outputs: A list of predicted elements\n",
    "        targets:  A list of elements that are to be predicted\n",
    "        eps: epsilon to avoid zero division\n",
    "        argmax_dim: int, that specifies dimension for argmax transformation\n",
    "            in case of scores/probabilities in ``outputs``\n",
    "        num_classes: int, that specifies number of classes if it known\n",
    "\n",
    "    Returns:\n",
    "        float: F_1 score\n",
    "    \"\"\"\n",
    "    targets = targets.sum(axis=-1)\n",
    "    ordinal_preds = []\n",
    "    outputs = nn.Sigmoid()(outputs)\n",
    "    n = 7\n",
    "    for i in range(n):\n",
    "        if i == 0:\n",
    "            ordinal_preds.append(1 - outputs[:, i])\n",
    "        elif i == n - 1:\n",
    "            ordinal_preds.append(outputs[:, i-1])\n",
    "        else:\n",
    "            ordinal_preds.append(outputs[:, i-1] - outputs[:, i])\n",
    "    outputs = torch.stack(ordinal_preds, -1)\n",
    "    score = fbeta_score(\n",
    "        outputs=outputs,\n",
    "        targets=targets,\n",
    "        beta=1,\n",
    "        eps=eps,\n",
    "        argmax_dim=argmax_dim,\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "\n",
    "    return -1.0 * score.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ui.neptune.ai/tezdhar/wind-speed/e/WIN-442\n",
      "1/20 * Epoch (train): 100% 11/11 [00:34<00:00,  3.14s/it, loss=0.348]\n",
      "1/20 * Epoch (valid): 100% 3/3 [00:11<00:00,  3.84s/it, loss=0.380]\n",
      "[2021-02-07 16:34:20,738] \n",
      "1/20 * Epoch 1 (_base): lr=0.0005 | momentum=0.9000\n",
      "1/20 * Epoch 1 (train): f1=-1.299e-01 | loss=0.5083\n",
      "1/20 * Epoch 1 (valid): f1=-1.293e-01 | loss=0.3595\n",
      "2/20 * Epoch (train): 100% 11/11 [00:33<00:00,  3.01s/it, loss=0.326]\n",
      "2/20 * Epoch (valid): 100% 3/3 [00:10<00:00,  3.64s/it, loss=0.350]\n",
      "[2021-02-07 16:35:15,509] \n",
      "2/20 * Epoch 2 (_base): lr=0.0005 | momentum=0.9000\n",
      "2/20 * Epoch 2 (train): f1=-1.517e-01 | loss=0.3364\n",
      "2/20 * Epoch 2 (valid): f1=-1.721e-01 | loss=0.3351\n",
      "3/20 * Epoch (train):   0% 0/11 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "device = utils.get_device()\n",
    "neptune_logger = NeptuneLogger(\n",
    "    api_token=os.environ[\"NEPTUNE_API_TOKEN\"],\n",
    "    project_name=\"tezdhar/wind-speed\",\n",
    "    name=\"wind_speed\",\n",
    "    params=hparams,\n",
    "    tags=[f\"fold_0\"],\n",
    "    upload_source_files=[\"nn_model1.ipynb\"],\n",
    ")\n",
    "\n",
    "trained_model, val_preds, test_preds = train_folds(train, test, cvlist, hparams, device, neptune_logger, cont_features, cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/val_preds_nn.npy\", val_preds)\n",
    "np.save(\"data/test_preds_nn.npy\", test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def process_preds(y_pred):\n",
    "    ordinal_preds = []\n",
    "    n = 7\n",
    "    for i in range(n):\n",
    "        if i == 0:\n",
    "            ordinal_preds.append(1 - y_pred[:, i])\n",
    "        elif i == n - 1:\n",
    "            ordinal_preds.append(y_pred[:, i-1])\n",
    "        else:\n",
    "            ordinal_preds.append(y_pred[:, i-1] - y_pred[:, i])\n",
    "    ordinal_preds = np.argmax(np.vstack(ordinal_preds).T, 1)\n",
    "    return ordinal_preds\n",
    "\n",
    "def ordinal_f1_score(y_true, y_pred, thresh=[0.9]):\n",
    "    y_true = np.sum(y_true, axis=1)\n",
    "    ordinal_preds = process_preds(y_pred)\n",
    "    print(sum(ordinal_preds == 7))\n",
    "    y_true[y_true == 7] = 0\n",
    "    ordinal_preds[ordinal_preds == 7] = 0\n",
    "    print(y_true, ordinal_preds)\n",
    "    return f1_score(y_true, ordinal_preds, average='macro')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_preds = predict(trained_model, vl_dl, cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vl = np.vstack(vl.apply(ordinal_encode, axis=1))\n",
    "ordinal_f1_score(y_vl, val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
